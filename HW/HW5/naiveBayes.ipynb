{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function loadDictionary.<locals>.<lambda> at 0x0000015F678C1158>, {'boring': 0, 'superb': 1, 'love': 2, 'likable': 3, 'waste': 4, 'top': 5, 'powerful': 6, 'entertaining': 7, 'rich': 8, 'visual': 9, 'familiar': 10, 'successful': 11, 'former': 12, 'latest': 13, 'previous': 14, 'similar': 15, 'dramatic': 16, 'personal': 17, 'popular': 18, 'effective': 19, 'certain': 20, 'difficult': 21, 'local': 22, 'hilarious': 23, 'happy': 24, 'early': 25, 'worse': 26, 'dark': 27, 'easy': 28, 'wonderful': 29, 'important': 30, 'wild': 31, 'serious': 32, 'recent': 33, 'impressive': 34, 'poor': 35, 'single': 36, 'less': 37, 'stupid': 38, 'deep': 39, 'emotional': 40, 'romantic': 41, 'possible': 42, 'classic': 43, 'simple': 44, 'fine': 45, 'strong': 46, 'short': 47, 'white': 48, 'beautiful': 49, 'evil': 50, 'obvious': 51, 'perfect': 52, 'worst': 53, 'major': 54, 'full': 55, 'enough': 56, 'nice': 57, 'dead': 58, 'able': 59, 'second': 60, 'wrong': 61, 'right': 62, 'comic': 63, 'final': 64, 'true': 65, 'human': 66, 'main': 67, 'small': 68, 'entire': 69, 'next': 70, 'sure': 71, 'several': 72, 'hard': 73, 'different': 74, 'long': 75, 'whole': 76, 'interesting': 77, 'high': 78, 'black': 79, 'american': 80, 'special': 81, 'better': 82, 'least': 83, 'funny': 84, 'original': 85, 'old': 86, 'young': 87, 'last': 88, 'real': 89, 'big': 90, 'best': 91, 'great': 92, 'much': 93, 'many': 94, 'new': 95, 'little': 96, 'bad': 97, 'first': 98, 'good': 99, 'UNK': 100})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "def loadDictionary(path):\n",
    "    vocab = open(path).read().split()\n",
    "#     print(vocab)\n",
    "    N = len(vocab)\n",
    "    wordToIdxMapping = collections.defaultdict(lambda: N-1)\n",
    "    for i, word in enumerate(vocab):\n",
    "        wordToIdxMapping[word] = i\n",
    "    return wordToIdxMapping\n",
    "wordToIdxMapping = loadDictionary(\"dictionary.txt\")\n",
    "print(wordToIdxMapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "defaultdict(<function loadDictionary.<locals>.<lambda> at 0x0000015F678C1158>, {'boring': 0, 'superb': 1, 'love': 2, 'likable': 3, 'waste': 4, 'top': 5, 'powerful': 6, 'entertaining': 7, 'rich': 8, 'visual': 9, 'familiar': 10, 'successful': 11, 'former': 12, 'latest': 13, 'previous': 14, 'similar': 15, 'dramatic': 16, 'personal': 17, 'popular': 18, 'effective': 19, 'certain': 20, 'difficult': 21, 'local': 22, 'hilarious': 23, 'happy': 24, 'early': 25, 'worse': 26, 'dark': 27, 'easy': 28, 'wonderful': 29, 'important': 30, 'wild': 31, 'serious': 32, 'recent': 33, 'impressive': 34, 'poor': 35, 'single': 36, 'less': 37, 'stupid': 38, 'deep': 39, 'emotional': 40, 'romantic': 41, 'possible': 42, 'classic': 43, 'simple': 44, 'fine': 45, 'strong': 46, 'short': 47, 'white': 48, 'beautiful': 49, 'evil': 50, 'obvious': 51, 'perfect': 52, 'worst': 53, 'major': 54, 'full': 55, 'enough': 56, 'nice': 57, 'dead': 58, 'able': 59, 'second': 60, 'wrong': 61, 'right': 62, 'comic': 63, 'final': 64, 'true': 65, 'human': 66, 'main': 67, 'small': 68, 'entire': 69, 'next': 70, 'sure': 71, 'several': 72, 'hard': 73, 'different': 74, 'long': 75, 'whole': 76, 'interesting': 77, 'high': 78, 'black': 79, 'american': 80, 'special': 81, 'better': 82, 'least': 83, 'funny': 84, 'original': 85, 'old': 86, 'young': 87, 'last': 88, 'real': 89, 'big': 90, 'best': 91, 'great': 92, 'much': 93, 'many': 94, 'new': 95, 'little': 96, 'bad': 97, 'first': 98, 'good': 99, 'UNK': 100})\n"
     ]
    }
   ],
   "source": [
    "wordToIdxMapping['love']\n",
    "print(len(wordToIdxMapping))\n",
    "print(wordToIdxMapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 2, 1, 2, 2, 772]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def transfer(fileDj, vocabulary):\n",
    "    tokens = open(fileDj).read().strip()\\\n",
    "                    .replace('loved', 'love').replace('loves', 'love').replace('loving', 'love')\\\n",
    "                    .split()\n",
    "#     print(tokens)\n",
    "#     print(len(vocabulary))\n",
    "    vector = [0] * len(vocabulary)\n",
    "    for word in tokens:\n",
    "        if word in vocabulary:\n",
    "            vector[vocabulary[word]] += 1\n",
    "        else:\n",
    "            vector[-1] += 1\n",
    "    return vector\n",
    "vector = transfer(\"data_sets/training_set/pos/cv000_29590.txt\", wordToIdxMapping)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "def loadData(Path):\n",
    "#     print(os.getcwd())\n",
    "    wordToIdxMapping = loadDictionary(\"dictionary.txt\")\n",
    "    Xtrain, ytrain = [], []\n",
    "    # train\n",
    "    trainPosPath = Path + '/training_set/pos'\n",
    "    for file in os.listdir(trainPosPath):\n",
    "        if file.endswith(\".txt\"):\n",
    "            Xtrain.append(transfer(os.path.join(trainPosPath, file), wordToIdxMapping))\n",
    "            ytrain.append(1)\n",
    "    trainNegPath = Path + '/training_set/neg'\n",
    "    for file in os.listdir(trainNegPath):\n",
    "        if file.endswith(\".txt\"):\n",
    "            Xtrain.append(transfer(os.path.join(trainNegPath, file), wordToIdxMapping))\n",
    "            ytrain.append(0)\n",
    "    Xtrain = np.array(Xtrain).T\n",
    "    ytrain = np.array(ytrain)\n",
    "#     print(Xtrain.shape)\n",
    "#     print(Xtrain)\n",
    "#     print(ytrain)\n",
    "    Xtest, ytest = [], []\n",
    "    testPosPath = Path + '/test_set/pos'\n",
    "    for file in os.listdir(testPosPath):\n",
    "        if file.endswith(\".txt\"):\n",
    "            Xtest.append(transfer(os.path.join(testPosPath, file), wordToIdxMapping))\n",
    "            ytest.append(1)\n",
    "    testNegPath = Path + '/test_set/neg'\n",
    "    for file in os.listdir(testNegPath):\n",
    "        if file.endswith(\".txt\"):\n",
    "            Xtest.append(transfer(os.path.join(testNegPath, file), wordToIdxMapping))\n",
    "            ytest.append(0)\n",
    "    Xtest = np.array(Xtest).T\n",
    "    ytest = np.array(ytest)\n",
    "    return Xtrain, Xtest, ytrain, ytest\n",
    "Xtrain, Xtest, ytrain, ytest = loadData('data_sets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 1400)\n",
      "(1400,)\n",
      "(101, 600)\n",
      "(600,)\n",
      "531175\n",
      "475680\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "print(Xtest.shape)\n",
    "print(ytest.shape)\n",
    "# print(np.sum(Xtrain, axis=0))\n",
    "# print(np.sum(Xtrain, axis=1))\n",
    "# print(ytrain[:700])\n",
    "print(sum(Xtrain[100, :700]))\n",
    "print(sum(Xtrain[100, 700:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xpos[ 772  748  439 1135  732  938  845  659  295  476  940  752  358  317\n",
      " 1239  643  449  896  509  603  378  667  540  870  971  836  575 1209\n",
      " 1105 1233  952 1435  883 1194  748  546  460  923  370  605  758 1174\n",
      "  817  568  813  878  920  692  649  391  479  628  344  967  552  766\n",
      "  525  186  259  963 1056  691 1178  346  759  925  527  577  821  652\n",
      "  638 1001  497  712 1608  275 1265  615  594  388  485  640  353  883\n",
      "  750  812 1026 1063  705  401  369  461  791  388  642  584  828  751\n",
      "  451 1045  626  849  708  767  570 1184  661  528  332  721 1033  274\n",
      " 1013  885  272 1363  988  742  776  399  935  704 1532 1543  714  485\n",
      "  798  649  447  457  408 1002  342  628  746  481  991  447  568  478\n",
      "  970  791  741 1227  403  274 1969  943  624  518  551  662  433  882\n",
      "  829  730  460  353 1064  752  679  312 1715 1376  643 1057  934  692\n",
      "  514 1051  351  765  596  424  606  578  457  894  787  520  597  628\n",
      "  676  688  286  689  729  567  440  704  586  522  591  560  481  539\n",
      "  744  376  327  878  440  640  927  976  519  748 1297  675  833  364\n",
      "  510 1423  381  276  448  754  541  329  497  427 1002  324  698  614\n",
      "  753  594  585  629  619  214  270 1126  747 1036  697  420  869  417\n",
      "  609  633 1051 1250  326  612  540  514  860  954 1060  452 1245  722\n",
      " 1311  323  693  503 1969  550  876  680  435  921 1102  545  510  626\n",
      "  630  803  494 1682  244  948  429  868 1362  996  759  243 1272  577\n",
      "  128  701 1045  642 1276  448  903  398  611  695  827  649  450  391\n",
      "  827  499  654  652 1142  605  862  392  787 1022  920  674  906  814\n",
      "  417 1441  920 1157  341  917  396 1110  518 1634  885  313 1354  738\n",
      "  398  394  720  633  337 1616 1528  337  574  802 1124 1067  822  930\n",
      "  976  738  379  499  658  966  607  519  535 1960  592  502  291  181\n",
      "  380  505  403  388  695  481  776  897  868  511  682  325  552  478\n",
      "  523  877  712  556  966  944  805 1195  292  588  674  538  620  629\n",
      "  416  720  485  736  846 1280  381  615  780  268  739 1026  301 1400\n",
      "  268  728  235  747  595  982 1105  821  271 1485  650  253  557  564\n",
      "  571  747  612  809 1087 1563  607  725 1299  756  326  621  406  517\n",
      "  653 1686  726  904  309  325  738  922  397  478  291  458 1017 1384\n",
      "  847 1390 1401  923  624 1644  925  924 1366  451  487 1030  497  689\n",
      "  429  533  480  831  484 1215  794  820  695  758  558  381  687  952\n",
      " 1126  737  616  519  675  973  709 1437 1186  257  602  301  609 1050\n",
      "  750  722  583  664  463  536  785  485  378  558  784  725  822  555\n",
      " 1252  389 1038  380  608  698 1038  694  419  525  820 1937  964 1227\n",
      "  654 1568 1810  687 1405 1189  691 1244 1981  302  480  407  838  250\n",
      "  881 1545  809  989  668  765  624 1040  725  500  804  745  535  840\n",
      " 1097  490  438  307  508  742  881  375  785  836  237  649  620 1300\n",
      "  895  567  881  728  807  932 2590 1034  840  459 1161 1240  899  921\n",
      " 1876  645  610 1357  420  346  786  840  349  835  780  960  731  908\n",
      "  753  659  592 1023  921  619  525  945 1047  783  481  950  682  441\n",
      "  479 1092  725  878  419  652  726  640  790 2482  267  748 1349 1902\n",
      "  686  881  807  616 1266  457  817  808  523  449 1562  445  668  534\n",
      "  702  608 1171  528 1214 1240  691  897  943  435  666  773  703  542\n",
      "  685  796  878 1245 1113  798  937  568 2157 1146  178 1055  890  455\n",
      " 1739 1011  601  810  718  950  703  734  440 1039  874  823  521  775\n",
      "  964  511 1374  745  572  940  655  701 1327 1474 1428  514  544  560\n",
      "  884 1596 1076  975  783  892 1064  833  697  484  730 1504  626  874\n",
      "  279  749  475  632  299  577 1655  945  588  774  815  590 1334  746]\n",
      "sum531175\n",
      "pos total:549135\n",
      "xneg[ 807  272  533  533  822  718  626  648  777  904  801  581  507 1055\n",
      "  578  754  692  765  485  800  773  578  731 1173  754  616  556 1396\n",
      "  578  351  535  918 1390  552  598  613  333  607  781  741  286  609\n",
      "  550  317  569  650  801  789  413  766  596  556  394 1269  711  490\n",
      "  502  218  279  361  501 1272  614  811  657 1097  691  708  493  748\n",
      "  969  811  797  722  558  373  345  681  669 1231  529  302  448  908\n",
      "  527  601  891  762  368  617 1328  481  630  214  772  439  541 1120\n",
      "  418  730 1950  792  540  842  316  859  360 2081  644  616  352 1046\n",
      "  672  728  254  736  527  384  813  376  779  346 1433  615  854  607\n",
      "  771  642  365  356  392 1313  306  384  825  619  828  567 1296  868\n",
      " 1729 1496  516  538  717  829  748  750  707  589  566  501  557  396\n",
      "  882  543  464  514  593  346  413  897  708  789  616  347  944  525\n",
      "  426  521  784  269 1050  296  629  644  658  184  598  654  639  455\n",
      "  784  722  451  577  781 1041  668  548  834  660  966  440  494  648\n",
      "  323  349  632  575 1575  276  803  381  719  219  502  956  749  601\n",
      "  877  537  664  501  430  559  335  897  269  778  620  705  493  994\n",
      "  522  750  777  843  799  483 1008 1042  749  486  598  707  623  669\n",
      "  648  356  464  581  453 1077 1167  676  745  326  265  684  969  848\n",
      "  780  644  698  720 1103  879  447 1047  716 1134  314  779  997 1115\n",
      "  660  575  472 1392  254  555  851  936  815  746  959  451  779  592\n",
      "  511 1230  288  680  532  273  493  554  437  238  460  296  796  590\n",
      "  211 1047  581  635  519  678  998  581  818  768  539  259  899  987\n",
      "  828  536  457  614  686  555 1022 1169  769  595  806  637  606 1005\n",
      "  590  811  469  595  286  733  478  169  582  709  459  679  884  796\n",
      "  757  454  915  510  293  772  470  186  459  659  751  306  795 1025\n",
      "  426  562  407  800  368  263 1413  331  984 1095  531  877  588  351\n",
      "  966 1070  617 1476 1017  824  448  481  492  898  592  534  718  227\n",
      "  885  926  499  778  396  529  582  439  535  484  418  552  505  826\n",
      "  816  528  907  476  207  565  448  593  469 1119 1135  864  452  603\n",
      "  709  731  492  557  445  466 1341  759  820  966  866  620  767  883\n",
      "  710  690  588 1050 1279  949  656 1411 2056  532  556  242  608  798\n",
      "  800  129  750  960  622  484  217 1107 1121  560  404  669  568  383\n",
      "  394  763  206  692  255  864  802  531  766  721  721 1048  287  452\n",
      "  741  456  934  683  485  667  630  901  682  668  767  828  397  968\n",
      "  534  227 1319  861  786  388  754  453 1198 1451  865  586  623 1296\n",
      "  682  784  469  603  927  574  467  577  303  946  587 1012  742  597\n",
      "  538  873   16  837  331  405  441 1289  635  647 1090  275  640  769\n",
      "  566  222  697  497  373  510 1310  568 1716  776  498  842  575  768\n",
      "  258 1027  613  585  824  613  660  899  906  467  667  447  669  474\n",
      "  483  420  686  611  780  995  986  730  845  379  652  751  342  541\n",
      "  794  703  816  206  853  762  667  741  893 1034  728  925  676  320\n",
      " 1096  771  833  457  236  930  631  691  447  364  363  666 1082  652\n",
      "  503  490  734  509 1281  705  751  821  418 1393  406  527  316  545\n",
      "  447  294  512  527  572  643  754 1370  874  341  730  565 1035  732\n",
      "  323  699  546 1144  663 1140  867 1715  672  829  763  358  988  545\n",
      "  849  467  494  636  803  971  818  912  675  984  562 1030  509  729\n",
      "  375  977 1053  814  639  661  405  607  879  754 1223  577 1337  479\n",
      "  518  778  656  517  373  759  429  338  564  377  595  949  410  180\n",
      "  470  604  280  721 1154  901  274  411  815  725  644  906  763  573\n",
      "  695  621  698  517 1521  874  571  487  314  665  836  731  773  697]\n",
      "sum475680\n",
      "neg total:491278\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    " def naiveBayesMulFeature_train(Xtrain, ytrain, alpha = 1.0):\n",
    "    \"\"\"\n",
    "    Xtrain: v*d, where v is the vocab size and d is the number of documents\n",
    "    \"\"\"\n",
    "    vocabSize = Xtrain.shape[0]\n",
    "    #pos\n",
    "    Xpos = Xtrain[:, ytrain==1]\n",
    "    print('xpos{}'.format(Xpos[100]))\n",
    "    print('sum{}'.format(np.sum(Xpos[100])))\n",
    "    lenToalPosDocs = np.sum(Xpos)\n",
    "    print('pos total:{}'.format(lenToalPosDocs))\n",
    "    thetaPos = np.array([(np.sum(Xpos[i]) + alpha)/(lenToalPosDocs + alpha*vocabSize) for i in range(vocabSize)])\n",
    "    #neg\n",
    "    Xneg = Xtrain[:, ytrain==0]\n",
    "    print('xneg{}'.format(Xneg[100]))\n",
    "    print('sum{}'.format(np.sum(Xneg[100])))\n",
    "    lenTotalNegDocs = np.sum(Xneg)\n",
    "    print('neg total:{}'.format(lenTotalNegDocs))\n",
    "    thetaNeg = np.array([(np.sum(Xneg[i]) + alpha)/(lenTotalNegDocs + alpha*vocabSize) for i in range(vocabSize)])    \n",
    "    return thetaPos, thetaNeg\n",
    "#     return thetaPos/np.sum(thetaPos), thetaNeg/np.sum(thetaNeg)\n",
    "\n",
    "thetaPos, thetaNeg = naiveBayesMulFeature_train(Xtrain, ytrain, alpha=1)\n",
    "print(np.sum(thetaPos))\n",
    "print(np.sum(thetaNeg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "101\n",
      "[1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1\n",
      " 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1\n",
      " 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1\n",
      " 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1\n",
      " 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1]\n",
      "0.765\n"
     ]
    }
   ],
   "source": [
    "def naiveBayesMulFeature_test(Xtest, ytest, thetaPos, thetaNeg):\n",
    "    yPredict = []\n",
    "    logThetaPos = np.log(thetaPos)\n",
    "    logThetaNeg = np.log(thetaNeg)\n",
    "    numOfDocs = Xtest.shape[1]\n",
    "    vocabSize = Xtest.shape[0]\n",
    "    for i in range(numOfDocs):\n",
    "        freqs = Xtest[:, i]\n",
    "        posLogP, negLogP = 0, 0\n",
    "        for j in range(vocabSize):\n",
    "            posLogP += freqs[j] * logThetaPos[j]\n",
    "            negLogP += freqs[j] * logThetaNeg[j]\n",
    "#         print('pos={}, neg={}'.format(posLogP, negLogP))\n",
    "        if posLogP >= negLogP:\n",
    "            yPredict.append(1)\n",
    "        else:\n",
    "            yPredict.append(0)\n",
    "#     print(yPredict)\n",
    "    yPredict = np.array(yPredict)\n",
    "    Accuracy = yPredict[yPredict == ytest].shape[0]/numOfDocs\n",
    "    return yPredict, Accuracy\n",
    "print(Xtest.shape[1])\n",
    "print(np.log(thetaPos).shape[0])\n",
    "# print(ytest)\n",
    "ypredict, acc = naiveBayesMulFeature_test(Xtest, ytest, thetaPos, thetaNeg)\n",
    "print(ypredict)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 1400)\n",
      "0.765\n",
      "[1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1\n",
      " 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1\n",
      " 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1\n",
      " 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1\n",
      " 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "print(Xtrain.shape)\n",
    "clf.fit(Xtrain.T, ytrain)\n",
    "score_sklearn = clf.score(Xtest.T, ytest)\n",
    "ypredict_sklearn = clf.predict(Xtest.T)\n",
    "print(score_sklearn)\n",
    "print(ypredict_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05128205 0.06552707 0.40883191 0.04131054 0.02279202 0.11680912\n",
      " 0.10683761 0.14814815 0.0982906  0.0982906  0.08404558 0.0968661\n",
      " 0.08119658 0.08974359 0.08547009 0.11253561 0.08974359 0.1011396\n",
      " 0.07834758 0.12108262 0.0954416  0.0982906  0.0968661  0.12535613\n",
      " 0.0968661  0.11823362 0.06125356 0.13817664 0.0997151  0.12393162\n",
      " 0.1011396  0.06267806 0.0968661  0.1011396  0.11538462 0.05840456\n",
      " 0.11396011 0.1965812  0.03846154 0.09259259 0.10541311 0.07834758\n",
      " 0.11253561 0.14672365 0.13960114 0.14245014 0.13390313 0.14672365\n",
      " 0.11396011 0.14672365 0.13247863 0.1025641  0.17236467 0.04558405\n",
      " 0.13960114 0.12535613 0.31766382 0.14529915 0.13532764 0.17236467\n",
      " 0.16096866 0.14102564 0.32193732 0.15669516 0.17663818 0.1951567\n",
      " 0.17094017 0.16809117 0.18376068 0.18376068 0.18376068 0.21367521\n",
      " 0.19230769 0.2037037  0.21509972 0.31054131 0.18233618 0.23646724\n",
      " 0.17948718 0.15099715 0.20940171 0.20512821 0.3048433  0.21937322\n",
      " 0.24216524 0.21652422 0.26638177 0.28205128 0.3048433  0.30769231\n",
      " 0.2977208  0.49287749 0.41452991 0.56267806 0.45726496 0.41452991\n",
      " 0.45584046 0.26210826 0.54273504 0.56552707 0.9985755 ]\n",
      "[0.18518519 0.01851852 0.34045584 0.02706553 0.10541311 0.11396011\n",
      " 0.04558405 0.0997151  0.05982906 0.04700855 0.08262108 0.07264957\n",
      " 0.06837607 0.07977208 0.07834758 0.06267806 0.05698006 0.04985755\n",
      " 0.07692308 0.05698006 0.07549858 0.08547009 0.08119658 0.05128205\n",
      " 0.07549858 0.13105413 0.15527066 0.08404558 0.08262108 0.05270655\n",
      " 0.06837607 0.05555556 0.09259259 0.08974359 0.07407407 0.12108262\n",
      " 0.07834758 0.16239316 0.15954416 0.08262108 0.07264957 0.08119658\n",
      " 0.10541311 0.0954416  0.08547009 0.1011396  0.07264957 0.10826211\n",
      " 0.08831909 0.08974359 0.11680912 0.14529915 0.06980057 0.19230769\n",
      " 0.11253561 0.12250712 0.33903134 0.12393162 0.14814815 0.12393162\n",
      " 0.15099715 0.16809117 0.25641026 0.11253561 0.12393162 0.0982906\n",
      " 0.11396011 0.17663818 0.13817664 0.14529915 0.18518519 0.19373219\n",
      " 0.14102564 0.17236467 0.12393162 0.28062678 0.20512821 0.24786325\n",
      " 0.15669516 0.13390313 0.12535613 0.16524217 0.36324786 0.32193732\n",
      " 0.25356125 0.2037037  0.26068376 0.20655271 0.29059829 0.28917379\n",
      " 0.31908832 0.35042735 0.26638177 0.54273504 0.31766382 0.34472934\n",
      " 0.45868946 0.48860399 0.49430199 0.57692308 0.9985755 ]\n"
     ]
    }
   ],
   "source": [
    "def naiveBayesBernFeature_train(Xtrain, ytrain):\n",
    "    \"\"\"\n",
    "    Xtrain: v*d, where v is the vocab size and d is the number of documents\n",
    "    \"\"\"\n",
    "    vocabSize = Xtrain.shape[0]\n",
    "    NumDocs = Xtrain.shape[1]\n",
    "    #pos\n",
    "    Xpos = Xtrain[:, ytrain==1]\n",
    "    thetaPosTrue = np.zeros(vocabSize)\n",
    "#     print('pos')\n",
    "    for i in range(vocabSize):\n",
    "        wordVector = Xpos[i]\n",
    "#         print('wordVecotr: {}'.format(wordVector))\n",
    "        thetaPosTrue[i] = (wordVector[wordVector > 0].shape[0] + 1) / (Xpos.shape[1] + 2)\n",
    "    #neg\n",
    "    Xneg = Xtrain[:, ytrain==0]\n",
    "    thetaNegTrue = np.zeros(vocabSize)\n",
    "#     print('neg')\n",
    "    for i in range(vocabSize):\n",
    "        wordVector = Xneg[i]\n",
    "#         print('wordVecotr: {}'.format(wordVector))\n",
    "        thetaNegTrue[i] = (wordVector[wordVector > 0].shape[0] + 1) / (Xneg.shape[1] + 2)\n",
    "    return thetaPosTrue, thetaNegTrue\n",
    "thetaPosTrue, thetaNegTrue = naiveBayesBernFeature_train(Xtrain, ytrain)\n",
    "print(thetaPosTrue)\n",
    "print(thetaNegTrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62\n"
     ]
    }
   ],
   "source": [
    "def naiveBayesBernFeature_test(Xtest, ytest, thetaPosTrue, thetaNegTrue):\n",
    "    yPredict = []\n",
    "    numDocs = Xtest.shape[1]\n",
    "    vocabSize = Xtest.shape[0]\n",
    "    for i in range(numDocs):\n",
    "        freqs = Xtest[:, i]\n",
    "        posP = negP = 1\n",
    "        for j in range(freqs.shape[0]):\n",
    "            f = freqs[j]\n",
    "            if f > 0:\n",
    "#                 posP *= thetaPosTrue[j]\n",
    "#                 negP *= thetaNegTrue[j]\n",
    "                posP += np.log(thetaPosTrue[j])\n",
    "                negP += np.log(thetaNegTrue[j])\n",
    "            else:\n",
    "#                 posP *= (1-thetaNegTrue[j])\n",
    "#                 negP *= (1-thetaNegTrue[j])\n",
    "                posP += np.log(1-thetaNegTrue[j])\n",
    "                negP += np.log(1-thetaNegTrue[j])\n",
    "        if posP > negP:\n",
    "            yPredict.append(1)\n",
    "        else:\n",
    "            yPredict.append(0)\n",
    "    yPredict = np.array(yPredict)\n",
    "    Accuracy = yPredict[yPredict == ytest].shape[0]/numDocs\n",
    "    return yPredict, Accuracy\n",
    "yPredict, acc = naiveBayesBernFeature_test(Xtest, ytest, thetaPosTrue, thetaNegTrue)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
