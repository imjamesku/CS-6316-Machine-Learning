{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.6154999999999999\n",
      "5 0.6275000000000001\n",
      "7 0.629\n",
      "9 0.626\n",
      "11 0.6285\n",
      "13 0.6255000000000001\n",
      "[3, 5, 7, 9, 11, 13]\n",
      "[0.6154999999999999, 0.6275000000000001, 0.629, 0.626, 0.6285, 0.6255000000000001]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2781b4197b8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Starting code for UVA CS 4501 Machine Learning- KNN\n",
    "\n",
    "__author__ = 'Jia-Jiun Ku'\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "np.random.seed(37)\n",
    "# for plot\n",
    "# more imports\n",
    "# the only purpose of the above import is in case that you want to compare your knn with sklearn knn\n",
    "\n",
    "\n",
    "# Load file into np arrays\n",
    "# x is the features\n",
    "# y is the labels\n",
    "def read_file(file):\n",
    "    data = np.loadtxt(file, skiprows=1)\n",
    "    np.random.shuffle(data)\n",
    "    x = data[:, :-1]\n",
    "    y = data[:, -1].astype(int)\n",
    "    return x, y\n",
    "\n",
    "# 2. Generate the i-th fold of k fold validation\n",
    "# Input:\n",
    "# x is an np array for training data\n",
    "# y is an np array for labels\n",
    "# i is an int indicating current fold\n",
    "# nfolds is the total number of cross validation folds\n",
    "\n",
    "\n",
    "def fold(x, y, i, nfolds):\n",
    "    # your code\n",
    "    num_examples = x.shape[0]\n",
    "    num_per_fold = num_examples//nfolds\n",
    "    test_start = i*num_per_fold\n",
    "    x_train = np.concatenate(\n",
    "        (x[0:test_start], x[test_start+num_per_fold:]), axis=0)\n",
    "    y_train = np.concatenate(\n",
    "        (y[0:test_start], y[test_start+num_per_fold:]), axis=0)\n",
    "    x_test = x[test_start: test_start+num_per_fold]\n",
    "    y_test = y[test_start: test_start+num_per_fold]\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# 3. Classify each testing points based on the training points\n",
    "# Input\n",
    "# x_train: a numpy array of training data\n",
    "# x_test: a numpy array\n",
    "# k: the number of neighbors to take into account when predicting the label\n",
    "# Output\n",
    "# y_predict: a numpy array\n",
    "\n",
    "\n",
    "def classify(x_train, y_train, x_test, k):\n",
    "    def distanceFrom(x_to_predict):\n",
    "        return lambda x: np.linalg.norm(x_to_predict - x[0])\n",
    "    # your code\n",
    "    # Euclidean distance as the measurement of distance in KNN\n",
    "    y_predict = []\n",
    "    for xi in x_test:\n",
    "        # sorted_x = [xi for xi, yi in sorted(zip(x_train, y_train), key=distanceFrom(xi))]\n",
    "        sorted_y = [yi for xi, yi in sorted(\n",
    "            zip(x_train, y_train), key=distanceFrom(xi))]\n",
    "        k_nearest = sorted_y[:k]\n",
    "        # print(k_nearest)\n",
    "        counts = Counter(k_nearest)\n",
    "        y_predict.append(counts.most_common(1)[0][0])\n",
    "    return y_predict\n",
    "\n",
    "# 4. Calculate accuracy by comaring with true labels\n",
    "# Input\n",
    "# y_predict is a numpy array of 1s and 0s for the class prediction\n",
    "# y is a numpy array of 1s and 0s for the true class label\n",
    "\n",
    "\n",
    "def calc_accuracy(y_predict, y):\n",
    "    # your code\n",
    "    count = 0\n",
    "    for y_predict_i, yi in zip(y_predict, y):\n",
    "        if y_predict_i == yi:\n",
    "            count += 1\n",
    "    acc = count/y.shape[0]\n",
    "    return acc\n",
    "\n",
    "# 5. Draw the bar plot of k vs. accuracy\n",
    "# klist: a list of values of ks\n",
    "# accuracy_list: a list of accuracies\n",
    "\n",
    "\n",
    "def barplot(klist, accuracy_list):\n",
    "    # your code\n",
    "    print(klist)\n",
    "    print(accuracy_list)\n",
    "    plt.bar(klist, accuracy_list)\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.6, 0.7])\n",
    "    plt.show()\n",
    "    # use matplot lib to generate bar plot with K on x axis and cross validation accuracy on y-axis\n",
    "    return\n",
    "\n",
    "# 1. Find the best K\n",
    "\n",
    "\n",
    "def findBestK(x, y, klist, nfolds):\n",
    "    kbest = 0\n",
    "    best_acc = 0\n",
    "    accuracy_list = []\n",
    "    for k in klist:\n",
    "        # your code here\n",
    "        n_fold_accuracy = []\n",
    "        for i in range(nfolds):\n",
    "            x_train, y_train, x_test, y_test = fold(x, y, i, nfolds)\n",
    "            y_predict = classify(x_train, y_train, x_test, k)\n",
    "            n_fold_accuracy.append(calc_accuracy(y_predict, y_test))\n",
    "        # to get nfolds cross validation accuracy for k neighbors\n",
    "        # implement fold(x, y, i, nfolds),classify(x_train, y_train, x_test, k) and calc_accuracy(y_predict, y)\n",
    "\n",
    "        # CROSS VALIDATION accuracy for k neighbors\n",
    "        accuracy = sum(n_fold_accuracy)/len(n_fold_accuracy)\n",
    "        if accuracy > best_acc:\n",
    "            kbest = k\n",
    "            best_acc = accuracy\n",
    "        accuracy_list.append(accuracy)\n",
    "        print(k, accuracy)\n",
    "    # plot cross validation error for each k : implement function barplot(klist, accuracy_list)\n",
    "    barplot(klist, accuracy_list)\n",
    "    return kbest\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filename = \"Movie_Review_Data.txt\"\n",
    "    # read data\n",
    "    x, y = read_file(filename)\n",
    "    nfolds = 4\n",
    "    klist = [3, 5, 7, 9, 11, 13]\n",
    "    # Implementation covers two tasks, both part of findBestK function\n",
    "    # Task 1 : implement kNN classifier for a given x,y,k\n",
    "    # Task 2 : implement 4 fold cross validation to select best k from klist\n",
    "\n",
    "    bestk = findBestK(x, y, klist, nfolds)\n",
    "    # report best k, and accuracy, discuss why some k work better than others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = cross_val_score(clf, x_train, y_train, cv=3)\n",
    "def cross_val_score(clf, x_train, y_train, cv):\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    num_per_fold = x_train.shape[0]//cv\n",
    "    for i in range(cv):\n",
    "#         x_train.iloc[np.r_[0:i*num_per_fold, (i+1)*num_per_fold:]]\n",
    "        x_train_folds = x_train.iloc[np.r_[0:i*num_per_fold, (i+1)*num_per_fold:]]\n",
    "        y_train_folds = y_train.iloc[np.r_[0:i*num_per_fold, (i+1)*num_per_fold:]]\n",
    "        x_test_fold = x_train.iloc[i*num_per_fold:(i+1)*num_per_fold]\n",
    "        y_test_fold = y_train.iloc[i*num_per_fold:(i+1)*num_per_fold]\n",
    "        clf.fit(x_train_folds, y_train_folds)\n",
    "        # Calculate test accuracy\n",
    "        y_predict = clf.predict(x_test_fold)\n",
    "        count = 0\n",
    "        for yi_train, yi_predict in zip(y_test_fold, y_predict):\n",
    "            if yi_train == yi_predict:\n",
    "                count += 1\n",
    "        test_scores.append(count/num_per_fold)\n",
    "        # Calculate test fold accuracy\n",
    "        y_predict = clf.predict(x_train_folds)\n",
    "        count = 0\n",
    "        for yi_train_folds, yi_predict in zip(y_train_folds, y_predict):\n",
    "            if yi_train_folds == yi_predict:\n",
    "                count += 1\n",
    "        train_scores.append(count/x_train_folds.shape[0])\n",
    "    return train_scores, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbf\n",
      "1\n",
      "rbf\n",
      "3\n",
      "rbf\n",
      "5\n",
      "rbf\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "param_set = [\n",
    "                 {'kernel': 'rbf', 'C': 1, 'degree': 1},\n",
    "                 {'kernel': 'rbf', 'C': 1, 'degree': 3},\n",
    "                 {'kernel': 'rbf', 'C': 1, 'degree': 5},\n",
    "                 {'kernel': 'rbf', 'C': 1, 'degree': 7},\n",
    "]\n",
    "for p in param_set:\n",
    "    print(p['kernel'])\n",
    "    print(p['degree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting code for UVA CS 4501 ML- SVM\n",
    "import pandas\n",
    "import sklearn\n",
    "import numpy as np\n",
    "np.random.seed(37)\n",
    "import random\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "# Att: You're not allowed to use modules other than SVC in sklearn, i.e., model_selection.\n",
    "\n",
    "# Dataset information\n",
    "# the column names (names of the features) in the data files\n",
    "# you can use this information to preprocess the features\n",
    "col_names_x = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "             'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "             'hours-per-week', 'native-country']\n",
    "col_names_y = ['label']\n",
    "\n",
    "numerical_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss',\n",
    "                  'hours-per-week']\n",
    "categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', 'relationship',\n",
    "                    'race', 'sex', 'native-country']\n",
    "\n",
    "# 1. Data loading from file and pre-processing.\n",
    "# Hint: Feel free to use some existing libraries for easier data pre-processing. \n",
    "# For example, as a start you can use one hot encoding for the categorical variables and normalization \n",
    "# for the continuous variables.\n",
    "def load_data(csv_file_path):\n",
    "    # your code here\n",
    "    df = pandas.read_csv(\"salary.labeled.csv\", names=col_names_x+col_names_y)\n",
    "    ## One-hot encoding\n",
    "    one_hot_df = pandas.get_dummies(df, columns=['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country'])\n",
    "    ## normalize\n",
    "    for col in ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']:\n",
    "        x, y = df[col].min(), df[col].max()\n",
    "        one_hot_df[col] = (one_hot_df[col] - x) / (y - x)\n",
    "#     print(one_hot_df)\n",
    "    one_hot_df.loc[:, 'label'] = one_hot_df.loc[:, 'label'].str.strip()\n",
    "#     print(one_hot_df.loc[0, 'label'] == '<=50K')\n",
    "#     print(one_hot_df.loc[0, 'label'])\n",
    "#     print(type(one_hot_df.loc[0, 'label']))\n",
    "    one_hot_df.loc[one_hot_df.label == '<=50K' , 'label'] = 0\n",
    "    one_hot_df.loc[one_hot_df.label == '>50K' , 'label'] = 1\n",
    "    \n",
    "    return one_hot_df.loc[:, one_hot_df.columns != 'label'], one_hot_df[col_names_y[0]]\n",
    "\n",
    "# 2. Select best hyperparameter with cross validation and train model.\n",
    "# Attention: Write your own hyper-parameter candidates.\n",
    "def train_and_select_model(training_csv):\n",
    "    # load data and preprocess from filename training_csv\n",
    "    x_train, y_train = load_data(training_csv)\n",
    "    # hard code hyperparameter configurations, an example:\n",
    "    param_set = [\n",
    "        {'kernel': 'rbf', 'C': 1, 'degree': 1},\n",
    "        {'kernel': 'rbf', 'C': 1, 'degree': 3},\n",
    "        {'kernel': 'rbf', 'C': 1, 'degree': 5},\n",
    "        {'kernel': 'linear', 'C': 1, 'degree': 1},\n",
    "        {'kernel': 'linear', 'C': 1, 'degree': 3},\n",
    "        {'kernel': 'linear', 'C': 1, 'degree': 5},\n",
    "        {'kernel': 'linear', 'C': 1, 'degree': 7},\n",
    "        {'kernel': 'poly', 'C': 1, 'degree': 1},\n",
    "        {'kernel': 'poly', 'C': 1, 'degree': 3},\n",
    "        {'kernel': 'poly', 'C': 1, 'degree': 5},\n",
    "        {'kernel': 'poly', 'C': 1, 'degree': 7},\n",
    "        {'kernel': 'sigmoid', 'C': 1, 'degree': 3},\n",
    "        {'kernel': 'sigmoid', 'C': 1, 'degree': 5},\n",
    "        {'kernel': 'sigmoid', 'C': 1, 'degree': 7},\n",
    "    ]\n",
    "    # your code here\n",
    "    scores = []\n",
    "    best_params = None\n",
    "    best_score = 0\n",
    "    for params in param_set:\n",
    "        clf = sklearn.svm.SVC(kernel=params['kernel'], C=params['C'], degree=params['degree'], gamma='auto')\n",
    "        # iterate over all hyperparameter configurations\n",
    "        # perform 3 FOLD cross validation\n",
    "        cv_train_scores, cv_test_scores = cross_val_score(clf, x_train, y_train, cv=3)\n",
    "        print(params)\n",
    "        print('cv_train_scores: {}'.format(cv_train_scores))\n",
    "        print('cv_test_scores: {}'.format(cv_test_scores))\n",
    "        mean_score = np.mean(cv_test_scores)\n",
    "        if mean_score > best_score:\n",
    "            best_params = params\n",
    "            best_score = mean_score\n",
    "        # print cv scores for every hyperparameter and include in pdf report\n",
    "    # select best hyperparameter from cv scores, retrain model\n",
    "    best_model = sklearn.svm.SVC(kernel=best_params['kernel'], C=best_params['C'], degree=best_params['degree'], gamma='auto')\n",
    "    best_model.fit(x_train, y_train)\n",
    "    return best_model, best_score\n",
    "\n",
    "# predict for data in filename test_csv using trained model\n",
    "def predict(test_csv, trained_model):\n",
    "    x_test, _ = load_data(test_csv)\n",
    "    predictions = trained_model.predict(x_test)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# save predictions on test data in desired format \n",
    "def output_results(predictions):\n",
    "    with open('predictions.txt', 'w') as f:\n",
    "        for pred in predictions:\n",
    "            if pred == 0:\n",
    "                f.write('<=50K\\n')\n",
    "            else:\n",
    "                f.write('>50K\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv_train_scores: [0.8261373291109909, 0.8296645297494916, 0.8302618367189311]\n",
      "cv_test_scores: [0.8261373291109909, 0.8320846528153241, 0.8300764655904843]\n",
      "cv_train_scores: [0.8261373291109909, 0.8296645297494916, 0.8302618367189311]\n",
      "cv_test_scores: [0.8261373291109909, 0.8320846528153241, 0.8300764655904843]\n",
      "cv_train_scores: [0.8261373291109909, 0.8296645297494916, 0.8302618367189311]\n",
      "cv_test_scores: [0.8261373291109909, 0.8320846528153241, 0.8300764655904843]\n",
      "cv_train_scores: [0.8530933807059551, 0.854174712288561, 0.853433227774774]\n",
      "cv_test_scores: [0.8530933807059551, 0.855256043871167, 0.8515486213022322]\n",
      "cv_train_scores: [0.8530933807059551, 0.854174712288561, 0.853433227774774]\n",
      "cv_test_scores: [0.8530933807059551, 0.855256043871167, 0.8515486213022322]\n",
      "cv_train_scores: [0.8530933807059551, 0.854174712288561, 0.853433227774774]\n",
      "cv_test_scores: [0.8530933807059551, 0.855256043871167, 0.8515486213022322]\n",
      "cv_train_scores: [0.8530933807059551, 0.854174712288561, 0.853433227774774]\n",
      "cv_test_scores: [0.8530933807059551, 0.855256043871167, 0.8515486213022322]\n",
      "cv_train_scores: [0.8269097088128524, 0.8379547385494709, 0.8398393450220129]\n",
      "cv_test_scores: [0.8269097088128524, 0.8416621611184059, 0.8385726423109601]\n",
      "cv_train_scores: [0.8313122731134626, 0.8365129631059962, 0.837491310728354]\n",
      "cv_test_scores: [0.8313122731134626, 0.8390360701320769, 0.836873406966865]\n",
      "cv_train_scores: [0.8323936046960686, 0.837980484539533, 0.8405499343477253]\n",
      "cv_test_scores: [0.8323936046960686, 0.8389588321618908, 0.8396539738935661]\n",
      "cv_train_scores: [0.8376457866687264, 0.8416879071084679, 0.8425117787904534]\n",
      "cv_test_scores: [0.8376457866687264, 0.8420483509693365, 0.8420483509693365]\n",
      "cv_train_scores: [0.6311886923611647, 0.757781725496254, 0.7586931335444504]\n",
      "cv_test_scores: [0.6311886923611647, 0.7556190623310419, 0.7609484822738859]\n",
      "cv_train_scores: [0.6311886923611647, 0.757781725496254, 0.7586931335444504]\n",
      "cv_test_scores: [0.6311886923611647, 0.7556190623310419, 0.7609484822738859]\n",
      "cv_train_scores: [0.6311886923611647, 0.757781725496254, 0.7586931335444504]\n",
      "cv_test_scores: [0.6311886923611647, 0.7556190623310419, 0.7609484822738859]\n",
      "The best model was scored 0.85\n"
     ]
    }
   ],
   "source": [
    "training_csv = \"salary.labeled.csv\"\n",
    "testing_csv = \"salary.2Predict.csv\"\n",
    "# fill in train_and_select_model(training_csv) to \n",
    "# return a trained model with best hyperparameter from 3-FOLD \n",
    "# cross validation to select hyperparameters as well as cross validation score for best hyperparameter. \n",
    "# hardcode hyperparameter configurations as part of train_and_select_model(training_csv)\n",
    "trained_model, cv_score = train_and_select_model(training_csv)\n",
    "\n",
    "print(\"The best model was scored %.2f\" % cv_score)\n",
    "# use trained SVC model to generate predictions\n",
    "predictions = predict(testing_csv, trained_model)\n",
    "# Don't archive the files or change the file names for the automated grading.\n",
    "# Do not shuffle the test dataset\n",
    "output_results(predictions)\n",
    "# 3. Upload your Python code, the predictions.txt as well as a report to Collab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'rbf', 'C': 1, 'degree': 1}\n",
      "[0.82630522 0.82522397 0.82705083]\n",
      "{'kernel': 'rbf', 'C': 1, 'degree': 3}\n",
      "[0.82630522 0.82522397 0.82705083]\n",
      "{'kernel': 'rbf', 'C': 1, 'degree': 5}\n",
      "[0.82630522 0.82522397 0.82705083]\n",
      "{'kernel': 'linear', 'C': 1, 'degree': 1}\n",
      "[0.85318196 0.85094223 0.85045574]\n",
      "{'kernel': 'linear', 'C': 1, 'degree': 3}\n",
      "[0.85318196 0.85094223 0.85045574]\n",
      "{'kernel': 'linear', 'C': 1, 'degree': 5}\n",
      "[0.85318196 0.85094223 0.85045574]\n",
      "{'kernel': 'linear', 'C': 1, 'degree': 7}\n",
      "[0.85318196 0.85094223 0.85045574]\n",
      "{'kernel': 'poly', 'C': 1, 'degree': 1}\n",
      "[0.82916281 0.8296262  0.83230341]\n",
      "{'kernel': 'poly', 'C': 1, 'degree': 3}\n",
      "[0.83387396 0.83070745 0.83469798]\n",
      "{'kernel': 'poly', 'C': 1, 'degree': 5}\n",
      "[0.83364226 0.83271548 0.83346207]\n",
      "{'kernel': 'poly', 'C': 1, 'degree': 7}\n",
      "[0.83379673 0.83402842 0.83477522]\n",
      "{'kernel': 'sigmoid', 'C': 1, 'degree': 3}\n",
      "[0.62735558 0.63044486 0.63170091]\n",
      "{'kernel': 'sigmoid', 'C': 1, 'degree': 5}\n",
      "[0.62735558 0.63044486 0.63170091]\n",
      "{'kernel': 'sigmoid', 'C': 1, 'degree': 7}\n",
      "[0.62735558 0.63044486 0.63170091]\n"
     ]
    }
   ],
   "source": [
    "# Library CV\n",
    "# lib: [0.82630522 0.82522397 0.82705083]\n",
    "x, y = load_data(\"salary.labeled.csv\")\n",
    "train_and_select_model(\"salary.labeled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38842, 15)\n",
      "age\n",
      "fnlwgt\n",
      "education-num\n",
      "capital-gain\n",
      "capital-loss\n",
      "hours-per-week\n",
      "label\n",
      "workclass_ ?\n",
      "workclass_ Federal-gov\n",
      "workclass_ Local-gov\n",
      "workclass_ Never-worked\n",
      "workclass_ Private\n",
      "workclass_ Self-emp-inc\n",
      "workclass_ Self-emp-not-inc\n",
      "workclass_ State-gov\n",
      "workclass_ Without-pay\n",
      "education_ 10th\n",
      "education_ 11th\n",
      "education_ 12th\n",
      "education_ 1st-4th\n",
      "education_ 5th-6th\n",
      "education_ 7th-8th\n",
      "education_ 9th\n",
      "education_ Assoc-acdm\n",
      "education_ Assoc-voc\n",
      "education_ Bachelors\n",
      "education_ Doctorate\n",
      "education_ HS-grad\n",
      "education_ Masters\n",
      "education_ Preschool\n",
      "education_ Prof-school\n",
      "education_ Some-college\n",
      "marital-status_ Divorced\n",
      "marital-status_ Married-AF-spouse\n",
      "marital-status_ Married-civ-spouse\n",
      "marital-status_ Married-spouse-absent\n",
      "marital-status_ Never-married\n",
      "marital-status_ Separated\n",
      "marital-status_ Widowed\n",
      "occupation_ ?\n",
      "occupation_ Adm-clerical\n",
      "occupation_ Armed-Forces\n",
      "occupation_ Craft-repair\n",
      "occupation_ Exec-managerial\n",
      "occupation_ Farming-fishing\n",
      "occupation_ Handlers-cleaners\n",
      "occupation_ Machine-op-inspct\n",
      "occupation_ Other-service\n",
      "occupation_ Priv-house-serv\n",
      "occupation_ Prof-specialty\n",
      "occupation_ Protective-serv\n",
      "occupation_ Sales\n",
      "occupation_ Tech-support\n",
      "occupation_ Transport-moving\n",
      "relationship_ Husband\n",
      "relationship_ Not-in-family\n",
      "relationship_ Other-relative\n",
      "relationship_ Own-child\n",
      "relationship_ Unmarried\n",
      "relationship_ Wife\n",
      "race_ Amer-Indian-Eskimo\n",
      "race_ Asian-Pac-Islander\n",
      "race_ Black\n",
      "race_ Other\n",
      "race_ White\n",
      "sex_ Female\n",
      "sex_ Male\n",
      "native-country_ ?\n",
      "native-country_ Cambodia\n",
      "native-country_ Canada\n",
      "native-country_ China\n",
      "native-country_ Columbia\n",
      "native-country_ Cuba\n",
      "native-country_ Dominican-Republic\n",
      "native-country_ Ecuador\n",
      "native-country_ El-Salvador\n",
      "native-country_ England\n",
      "native-country_ France\n",
      "native-country_ Germany\n",
      "native-country_ Greece\n",
      "native-country_ Guatemala\n",
      "native-country_ Haiti\n",
      "native-country_ Honduras\n",
      "native-country_ Hong\n",
      "native-country_ Hungary\n",
      "native-country_ India\n",
      "native-country_ Iran\n",
      "native-country_ Ireland\n",
      "native-country_ Italy\n",
      "native-country_ Jamaica\n",
      "native-country_ Japan\n",
      "native-country_ Laos\n",
      "native-country_ Mexico\n",
      "native-country_ Nicaragua\n",
      "native-country_ Outlying-US(Guam-USVI-etc)\n",
      "native-country_ Peru\n",
      "native-country_ Philippines\n",
      "native-country_ Poland\n",
      "native-country_ Portugal\n",
      "native-country_ Puerto-Rico\n",
      "native-country_ Scotland\n",
      "native-country_ South\n",
      "native-country_ Taiwan\n",
      "native-country_ Thailand\n",
      "native-country_ Trinadad&Tobago\n",
      "native-country_ United-States\n",
      "native-country_ Vietnam\n",
      "native-country_ Yugoslavia\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.630137</td>\n",
       "      <td>0.129835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.260274</td>\n",
       "      <td>0.146818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.124841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.753425</td>\n",
       "      <td>0.127214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0.122417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38837</td>\n",
       "      <td>0.287671</td>\n",
       "      <td>0.045237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.602041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38838</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.275011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38839</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.206967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38840</td>\n",
       "      <td>0.273973</td>\n",
       "      <td>0.076681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38841</td>\n",
       "      <td>0.301370</td>\n",
       "      <td>0.195053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38842 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    fnlwgt  capital-gain  capital-loss  hours-per-week\n",
       "0      0.630137  0.129835           0.0           0.0        0.397959\n",
       "1      0.260274  0.146818           0.0           0.0        0.500000\n",
       "2      0.109589  0.124841           0.0           0.0        0.500000\n",
       "3      0.753425  0.127214           0.0           0.0        0.397959\n",
       "4      0.520548  0.122417           0.0           0.0        0.397959\n",
       "...         ...       ...           ...           ...             ...\n",
       "38837  0.287671  0.045237           0.0           0.0        0.602041\n",
       "38838  0.109589  0.275011           0.0           0.0        0.071429\n",
       "38839  0.054795  0.206967           0.0           0.0        0.397959\n",
       "38840  0.273973  0.076681           0.0           0.0        0.530612\n",
       "38841  0.301370  0.195053           0.0           0.0        0.397959\n",
       "\n",
       "[38842 rows x 5 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.read_csv(\"salary.labeled.csv\", names=col_names_x+col_names_y)\n",
    "print(df.shape)\n",
    "df.iloc[0]\n",
    "df.loc[:,col_names_x]\n",
    "## One-hot encoding\n",
    "one_hot_df = pandas.get_dummies(df, columns=['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country'])\n",
    "## normalize\n",
    "for col in ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']:\n",
    "    x, y = df[col].min(), df[col].max()\n",
    "    one_hot_df[col] = (one_hot_df[col] - x) / (y - x)\n",
    "for col in one_hot_df.columns:\n",
    "    print(col)\n",
    "# print(one_hot_df.shape)\n",
    "one_hot_df[ ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']]\n",
    "# df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    training_csv = \"salary.labeled.csv\"\n",
    "    testing_csv = \"salary.2Predict.csv\"\n",
    "    # fill in train_and_select_model(training_csv) to \n",
    "    # return a trained model with best hyperparameter from 3-FOLD \n",
    "    # cross validation to select hyperparameters as well as cross validation score for best hyperparameter. \n",
    "    # hardcode hyperparameter configurations as part of train_and_select_model(training_csv)\n",
    "    trained_model, cv_score = train_and_select_model(training_csv)\n",
    "\n",
    "    print \"The best model was scored %.2f\" % cv_score\n",
    "    # use trained SVC model to generate predictions\n",
    "    predictions = predict(testing_csv, trained_model)\n",
    "    # Don't archive the files or change the file names for the automated grading.\n",
    "    # Do not shuffle the test dataset\n",
    "    output_results(predictions)\n",
    "    # 3. Upload your Python code, the predictions.txt as well as a report to Collab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
